---
title: "Scraping de LinkedIn con Python y Proxycurl API"
description: Aprende cómo hacer scraping de perfiles de LinkedIn usando Python y Proxycurl API, incluyendo configuración, configuración de API y manejo de respuestas
tags: ["web-scraping", "python"]
date: 2024-08-21
published: true
cover: "./images/cover/linkedin-scraping-with-python-and-proxycurl-api.avif"
---

Hacer scraping de datos de perfiles de LinkedIn puede ser increíblemente útil para varios propósitos, desde investigación hasta crear currículums. En este artículo, te guiaré a través del proceso de usar Python junto con la API de Proxycurl para hacer scraping de datos de un perfil de LinkedIn.

### Paso 1: Configura tu Cuenta de Proxycurl

Antes de sumergirte en el código, necesitarás crear una cuenta en el [sitio web de Proxycurl](https://nubela.co/proxycurl/). Después de registrarte, recibirás una clave API, que es esencial para acceder a la API de Proxycurl.

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1724180135536/7cb08f64-b07f-4983-b7b2-a69c480f9ed4.png)

### Paso 2: Almacena tu Clave API de Forma Segura

Para mantener tu clave API segura, es buena práctica almacenarla en un archivo `.env`. Este archivo se usa para mantener información sensible, como claves API, fuera de tu código base. Así es como puedes configurar tu archivo `.env`:

```plaintext
API_KEY=tu_clave_api_aqui
```

Reemplaza `tu_clave_api_aqui` con la clave API real que recibiste de Proxycurl.

### Paso 3: Escribiendo el Script de Python

Ahora, veamos el código Python paso a paso. Primero, necesitarás importar las librerías necesarias:

```python
import requests
import json
from dotenv import load_dotenv
import os
```

- **requests**: Esta librería te permite enviar solicitudes HTTP en Python, que es cómo interactuaremos con la API de Proxycurl.
- **json**: Usado para manejar datos JSON, que es el formato que devolverá la API.
- **dotenv**: Ayuda a cargar variables de entorno desde un archivo `.env`.
- **os**: Proporciona funciones para interactuar con el sistema operativo, como acceder a variables de entorno.

Luego, cargamos la clave API desde el archivo `.env`:

La función `load_dotenv()` carga las variables de entorno desde el archivo `.env`, y `os.getenv("API_KEY")` recupera el valor de la variable `API_KEY`.

### Paso 4: Configurando la URL del Perfil de LinkedIn y el Endpoint de la API

Ahora, necesitas especificar la URL del perfil de LinkedIn que quieres hacer scraping y el endpoint de la API de Proxycurl:

```python
linkedin_profile_url = "https://www.linkedin.com/in/ayush-that/"
api_endpoint = "https://nubela.co/proxycurl/api/v2/linkedin"
```

- **linkedin_profile_url**: Esta es la URL del perfil de LinkedIn del que quieres hacer scraping.
- **api_endpoint**: Este es el endpoint de la API de Proxycurl para hacer scraping de datos de perfiles de LinkedIn.

### Paso 5: Configurando la Solicitud de la API

Para personalizar los datos que quieres hacer scraping, necesitas configurar los parámetros y headers para la solicitud de la API:

```python
params = {
    "url": linkedin_profile_url,
    "fallback_to_cache": "on-error",
    "use_cache": "if-present",
    "skills": "include",
    "inferred_salary": "include",
    "extra": "include",
    "personal_email": "include",
}
headers = {
    "Authorization": f"Bearer {API_KEY}",
}
```

- **params**: Estos parámetros controlan qué datos devuelve la API. Por ejemplo, `skills` incluye las habilidades del perfil, `inferred_salary` proporciona una estimación del salario del perfil, y `personal_email` incluye el email si está disponible.
- **headers**: Esto incluye el header `Authorization`, que pasa tu clave API a la API para autenticación.

### Paso 6: Enviando la Solicitud y Manejando la Respuesta

Ahora, enviamos la solicitud a la API de Proxycurl y manejamos la respuesta:

```python
response = requests.get(api_endpoint, params=params, headers=headers)
```

Esta línea envía una solicitud GET a la API con los parámetros y headers especificados. La API entonces devolverá los datos del perfil en formato JSON.

Luego, verificamos si la solicitud fue exitosa y guardamos los datos:

```python
if response.status_code == 200:
    profile_data = response.json()
    with open("profile_data.json", "w") as json_file:
        json.dump(profile_data, json_file, indent=4)
else:
    print(f"Error: {response.status_code}")
```

- **response.status_code**: Esto verifica el estado de la solicitud. Un código de estado de 200 significa que la solicitud fue exitosa.
- **profile_data.json**: Si es exitosa, los datos del perfil se guardan en un archivo llamado `profile_data.json`.

### Paso 7: Ejecutando el Script

Guarda tu script como [`app.py`](http://app.py) y ejecútalo usando el siguiente comando:

```bash
python app.py
```

Después de ejecutar el script, encontrarás un archivo llamado `profile_data.json` en tu directorio. Este archivo contiene todos los datos extraídos del perfil de LinkedIn en un formato estructurado.

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1724180047004/7429386a-6b3c-43c2-a0b5-44f81ea29d6e.png)

### Conclusión

¡Y eso es todo! Has hecho scraping exitosamente de datos de un perfil de LinkedIn usando Python y la API de Proxycurl. En el próximo tutorial, exploraremos cómo usar estos datos extraídos para crear un currículum generado por IA y amigable con ATS.

Para el código completo, puedes visitar [GitHub](https://github.com/ayush-that/ResumeGenie).

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1724179604343/0e0ee533-6b25-47b4-9207-8e1afd41a82a.png)
