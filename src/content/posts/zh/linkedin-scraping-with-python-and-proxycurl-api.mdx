---
title: "使用Python和Proxycurl API抓取LinkedIn数据"
description: 学习如何使用Python和Proxycurl API抓取LinkedIn个人资料，包括设置、API配置和处理响应
tags: ["web-scraping", "python"]
date: 2024-08-21
published: true
cover: "./images/cover/linkedin-scraping-with-python-and-proxycurl-api.avif"
---

从LinkedIn个人资料中抓取数据对于各种目的都非常有用，从研究到制作简历。在这篇文章中，我将带你了解使用Python和Proxycurl API从LinkedIn个人资料中抓取数据的过程。

### 第1步：设置你的Proxycurl账户

在深入代码之前，你需要在[Proxycurl网站](https://nubela.co/proxycurl/)上创建一个账户。注册后，你将收到一个API密钥，这对于访问Proxycurl API至关重要。

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1724180135536/7cb08f64-b07f-4983-b7b2-a69c480f9ed4.png)

### 第2步：安全存储你的API密钥

为了保护你的API密钥安全，最好将其存储在`.env`文件中。此文件用于将API密钥等敏感信息保存在代码库之外。以下是如何设置你的`.env`文件：

```plaintext
API_KEY=your_api_key_here
```

将`your_api_key_here`替换为你从Proxycurl收到的实际API密钥。

### 第3步：编写Python脚本

现在，让我们逐步了解Python代码。首先，你需要导入必要的库：

```python
import requests
import json
from dotenv import load_dotenv
import os
```

- **requests**：这个库允许你在Python中发送HTTP请求，这是我们与Proxycurl API交互的方式。
- **json**：用于处理JSON数据，这是API返回的格式。
- **dotenv**：帮助从`.env`文件加载环境变量。
- **os**：提供与操作系统交互的函数，例如访问环境变量。

接下来，我们从`.env`文件加载API密钥：

`load_dotenv()`函数从`.env`文件加载环境变量，`os.getenv("API_KEY")`检索`API_KEY`变量的值。

### 第4步：设置LinkedIn个人资料URL和API端点

现在，你需要指定要抓取的LinkedIn个人资料URL和Proxycurl API端点：

```python
linkedin_profile_url = "https://www.linkedin.com/in/ayush-that/"
api_endpoint = "https://nubela.co/proxycurl/api/v2/linkedin"
```

- **linkedin_profile_url**：这是你要抓取的LinkedIn个人资料的URL。
- **api_endpoint**：这是用于抓取LinkedIn个人资料数据的Proxycurl API端点。

### 第5步：配置API请求

要自定义你想要抓取的数据，你需要设置API请求的参数和标头：

```python
params = {
    "url": linkedin_profile_url,
    "fallback_to_cache": "on-error",
    "use_cache": "if-present",
    "skills": "include",
    "inferred_salary": "include",
    "extra": "include",
    "personal_email": "include",
}
headers = {
    "Authorization": f"Bearer {API_KEY}",
}
```

- **params**：这些参数控制API返回的数据。例如，`skills`包含个人资料的技能，`inferred_salary`提供个人资料薪资的估计，`personal_email`如果可用则包含电子邮件。
- **headers**：这包括`Authorization`标头，将你的API密钥传递给API进行身份验证。

### 第6步：发送请求和处理响应

现在，我们向Proxycurl API发送请求并处理响应：

```python
response = requests.get(api_endpoint, params=params, headers=headers)
```

这行代码使用指定的参数和标头向API发送GET请求。然后API将以JSON格式返回个人资料数据。

接下来，我们检查请求是否成功并保存数据：

```python
if response.status_code == 200:
    profile_data = response.json()
    with open("profile_data.json", "w") as json_file:
        json.dump(profile_data, json_file, indent=4)
else:
    print(f"Error: {response.status_code}")
```

- **response.status_code**：这检查请求的状态。状态码200表示请求成功。
- **profile_data.json**：如果成功，个人资料数据将保存到名为`profile_data.json`的文件中。

### 第7步：运行脚本

将你的脚本保存为[`app.py`](http://app.py)并使用以下命令运行：

```bash
python app.py
```

运行脚本后，你会在目录中找到一个名为`profile_data.json`的文件。此文件以结构化格式包含从LinkedIn个人资料抓取的所有数据。

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1724180047004/7429386a-6b3c-43c2-a0b5-44f81ea29d6e.png)

### 结论

就是这样！你已经成功使用Python和Proxycurl API从LinkedIn个人资料抓取了数据。在下一个教程中，我们将探索如何使用这些抓取的数据来创建AI生成的、ATS友好的简历。

完整代码请访问[GitHub](https://github.com/ayush-that/ResumeGenie)。

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1724179604343/0e0ee533-6b25-47b4-9207-8e1afd41a82a.png)
