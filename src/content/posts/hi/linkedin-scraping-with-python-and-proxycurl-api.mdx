---
title: "Python और Proxycurl API के साथ LinkedIn स्क्रैपिंग"
description: Python और Proxycurl API का उपयोग करके LinkedIn प्रोफाइल स्क्रैप करना सीखें, जिसमें सेटअप, API कॉन्फ़िगरेशन और रिस्पॉन्स हैंडलिंग शामिल है
tags: ["web-scraping", "python"]
date: 2024-08-21
published: true
cover: "./images/cover/linkedin-scraping-with-python-and-proxycurl-api.avif"
---

LinkedIn प्रोफाइल से डेटा स्क्रैप करना विभिन्न उद्देश्यों के लिए अविश्वसनीय रूप से उपयोगी हो सकता है, रिसर्च से लेकर रिज़्यूमे बनाने तक। इस आर्टिकल में, मैं आपको Python के साथ Proxycurl API का उपयोग करके LinkedIn प्रोफाइल से डेटा स्क्रैप करने की प्रक्रिया के बारे में बताऊंगा।

### स्टेप 1: अपना Proxycurl अकाउंट सेट करें

कोड में जाने से पहले, आपको [Proxycurl वेबसाइट](https://nubela.co/proxycurl/) पर एक अकाउंट बनाना होगा। रजिस्टर करने के बाद, आपको एक API key मिलेगी, जो Proxycurl API एक्सेस करने के लिए आवश्यक है।

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1724180135536/7cb08f64-b07f-4983-b7b2-a69c480f9ed4.png)

### स्टेप 2: अपनी API Key को सुरक्षित रूप से स्टोर करें

अपनी API key को सुरक्षित रखने के लिए, इसे `.env` फाइल में स्टोर करना एक अच्छा अभ्यास है। इस फाइल का उपयोग संवेदनशील जानकारी, जैसे API keys, को आपके कोडबेस से बाहर रखने के लिए किया जाता है। यहां बताया गया है कि आप अपनी `.env` फाइल कैसे सेट कर सकते हैं:

```plaintext
API_KEY=your_api_key_here
```

`your_api_key_here` को Proxycurl से प्राप्त वास्तविक API key से बदलें।

### स्टेप 3: Python स्क्रिप्ट लिखना

अब, आइए Python कोड को स्टेप बाय स्टेप देखें। सबसे पहले, आपको आवश्यक लाइब्रेरीज़ इम्पोर्ट करनी होंगी:

```python
import requests
import json
from dotenv import load_dotenv
import os
```

- **requests**: यह लाइब्रेरी आपको Python में HTTP रिक्वेस्ट भेजने की अनुमति देती है, जिससे हम Proxycurl API के साथ इंटरैक्ट करेंगे।
- **json**: JSON डेटा को हैंडल करने के लिए उपयोग किया जाता है, जो API द्वारा रिटर्न किया जाने वाला फॉर्मेट है।
- **dotenv**: `.env` फाइल से एनवायरनमेंट वेरिएबल्स लोड करने में मदद करता है।
- **os**: ऑपरेटिंग सिस्टम के साथ इंटरैक्ट करने के लिए फ़ंक्शन प्रदान करता है, जैसे एनवायरनमेंट वेरिएबल्स एक्सेस करना।

इसके बाद, हम `.env` फाइल से API key लोड करते हैं:

`load_dotenv()` फ़ंक्शन `.env` फाइल से एनवायरनमेंट वेरिएबल्स लोड करता है, और `os.getenv("API_KEY")` `API_KEY` वेरिएबल की वैल्यू प्राप्त करता है।

### स्टेप 4: LinkedIn प्रोफाइल URL और API एंडपॉइंट सेट करना

अब, आपको उस LinkedIn प्रोफाइल URL को स्पेसिफाई करना होगा जिसे आप स्क्रैप करना चाहते हैं और Proxycurl API एंडपॉइंट:

```python
linkedin_profile_url = "https://www.linkedin.com/in/ayush-that/"
api_endpoint = "https://nubela.co/proxycurl/api/v2/linkedin"
```

- **linkedin_profile_url**: यह उस LinkedIn प्रोफाइल का URL है जिसे आप स्क्रैप करना चाहते हैं।
- **api_endpoint**: यह LinkedIn प्रोफाइल डेटा स्क्रैप करने के लिए Proxycurl API एंडपॉइंट है।

### स्टेप 5: API रिक्वेस्ट कॉन्फ़िगर करना

जिस डेटा को आप स्क्रैप करना चाहते हैं उसे कस्टमाइज़ करने के लिए, आपको API रिक्वेस्ट के लिए पैरामीटर और हेडर सेट करने होंगे:

```python
params = {
    "url": linkedin_profile_url,
    "fallback_to_cache": "on-error",
    "use_cache": "if-present",
    "skills": "include",
    "inferred_salary": "include",
    "extra": "include",
    "personal_email": "include",
}
headers = {
    "Authorization": f"Bearer {API_KEY}",
}
```

- **params**: ये पैरामीटर नियंत्रित करते हैं कि API द्वारा कौन सा डेटा रिटर्न किया जाता है। उदाहरण के लिए, `skills` में प्रोफाइल की स्किल्स शामिल होती हैं, `inferred_salary` प्रोफाइल की सैलरी का अनुमान प्रदान करता है, और `personal_email` में उपलब्ध होने पर ईमेल शामिल होता है।
- **headers**: इसमें `Authorization` हेडर शामिल है, जो ऑथेंटिकेशन के लिए आपकी API key को API को पास करता है।

### स्टेप 6: रिक्वेस्ट भेजना और रिस्पॉन्स हैंडल करना

अब, हम Proxycurl API को रिक्वेस्ट भेजते हैं और रिस्पॉन्स को हैंडल करते हैं:

```python
response = requests.get(api_endpoint, params=params, headers=headers)
```

यह लाइन स्पेसिफाइड पैरामीटर और हेडर के साथ API को GET रिक्वेस्ट भेजती है। API फिर JSON फॉर्मेट में प्रोफाइल डेटा रिटर्न करेगा।

इसके बाद, हम चेक करते हैं कि रिक्वेस्ट सफल थी या नहीं और डेटा को सेव करते हैं:

```python
if response.status_code == 200:
    profile_data = response.json()
    with open("profile_data.json", "w") as json_file:
        json.dump(profile_data, json_file, indent=4)
else:
    print(f"Error: {response.status_code}")
```

- **response.status_code**: यह रिक्वेस्ट की स्थिति की जांच करता है। 200 का स्टेटस कोड मतलब रिक्वेस्ट सफल थी।
- **profile_data.json**: यदि सफल होता है, तो प्रोफाइल डेटा `profile_data.json` नामक फाइल में सेव हो जाता है।

### स्टेप 7: स्क्रिप्ट चलाना

अपनी स्क्रिप्ट को [`app.py`](http://app.py) के रूप में सेव करें और इसे निम्न कमांड का उपयोग करके चलाएं:

```bash
python app.py
```

स्क्रिप्ट चलाने के बाद, आपको अपनी डायरेक्टरी में `profile_data.json` नामक फाइल मिलेगी। इस फाइल में LinkedIn प्रोफाइल से स्क्रैप किया गया सारा डेटा एक स्ट्रक्चर्ड फॉर्मेट में होता है।

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1724180047004/7429386a-6b3c-43c2-a0b5-44f81ea29d6e.png)

### निष्कर्ष

बस इतना ही! आपने Python और Proxycurl API का उपयोग करके LinkedIn प्रोफाइल से डेटा सफलतापूर्वक स्क्रैप कर लिया है। अगले ट्यूटोरियल में, हम इस स्क्रैप किए गए डेटा का उपयोग AI-जनरेटेड, ATS-फ्रेंडली रिज़्यूमे बनाने के लिए कैसे करें, यह जानेंगे।

पूरे कोड के लिए, आप [GitHub](https://github.com/ayush-that/ResumeGenie) पर जा सकते हैं।

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1724179604343/0e0ee533-6b25-47b4-9207-8e1afd41a82a.png)
