---
title: "Python과 Proxycurl API로 LinkedIn 스크래핑"
description: Python과 Proxycurl API를 사용하여 LinkedIn 프로필을 스크래핑하는 방법을 배웁니다. 설정, API 구성 및 응답 처리 포함
tags: ["web-scraping", "python"]
date: 2024-08-21
published: true
cover: "./images/cover/linkedin-scraping-with-python-and-proxycurl-api.avif"
---

LinkedIn 프로필에서 데이터를 스크래핑하는 것은 연구부터 이력서 작성까지 다양한 목적에 매우 유용할 수 있습니다. 이 글에서는 Python과 Proxycurl API를 사용하여 LinkedIn 프로필에서 데이터를 스크래핑하는 과정을 안내합니다.

### 1단계: Proxycurl 계정 설정

코드에 들어가기 전에 [Proxycurl 웹사이트](https://nubela.co/proxycurl/)에서 계정을 만들어야 합니다. 등록 후 Proxycurl API에 접근하는 데 필수적인 API 키를 받게 됩니다.

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1724180135536/7cb08f64-b07f-4983-b7b2-a69c480f9ed4.png)

### 2단계: API 키 안전하게 저장

API 키를 안전하게 보관하려면 `.env` 파일에 저장하는 것이 좋습니다. 이 파일은 API 키와 같은 민감한 정보를 코드베이스에서 분리하는 데 사용됩니다. `.env` 파일 설정 방법:

```plaintext
API_KEY=your_api_key_here
```

`your_api_key_here`를 Proxycurl에서 받은 실제 API 키로 바꾸세요.

### 3단계: Python 스크립트 작성

이제 Python 코드를 단계별로 살펴보겠습니다. 먼저 필요한 라이브러리를 임포트해야 합니다:

```python
import requests
import json
from dotenv import load_dotenv
import os
```

- **requests**: Python에서 HTTP 요청을 보낼 수 있게 해주는 라이브러리로, Proxycurl API와 상호작용하는 데 사용합니다.
- **json**: API가 반환하는 형식인 JSON 데이터를 처리하는 데 사용합니다.
- **dotenv**: `.env` 파일에서 환경 변수를 로드하는 데 도움이 됩니다.
- **os**: 환경 변수 접근과 같은 운영 체제와의 상호작용을 위한 함수를 제공합니다.

다음으로 `.env` 파일에서 API 키를 로드합니다:

`load_dotenv()` 함수는 `.env` 파일에서 환경 변수를 로드하고, `os.getenv("API_KEY")`는 `API_KEY` 변수의 값을 가져옵니다.

### 4단계: LinkedIn 프로필 URL 및 API 엔드포인트 설정

스크래핑하려는 LinkedIn 프로필 URL과 Proxycurl API 엔드포인트를 지정해야 합니다:

```python
linkedin_profile_url = "https://www.linkedin.com/in/ayush-that/"
api_endpoint = "https://nubela.co/proxycurl/api/v2/linkedin"
```

- **linkedin_profile_url**: 스크래핑하려는 LinkedIn 프로필의 URL입니다.
- **api_endpoint**: LinkedIn 프로필 데이터를 스크래핑하기 위한 Proxycurl API 엔드포인트입니다.

### 5단계: API 요청 구성

스크래핑하려는 데이터를 사용자 정의하려면 API 요청에 대한 매개변수와 헤더를 설정해야 합니다:

```python
params = {
    "url": linkedin_profile_url,
    "fallback_to_cache": "on-error",
    "use_cache": "if-present",
    "skills": "include",
    "inferred_salary": "include",
    "extra": "include",
    "personal_email": "include",
}
headers = {
    "Authorization": f"Bearer {API_KEY}",
}
```

- **params**: 이러한 매개변수는 API가 반환하는 데이터를 제어합니다. 예를 들어 `skills`는 프로필의 기술을 포함하고, `inferred_salary`는 프로필의 급여 추정치를 제공하며, `personal_email`은 가능한 경우 이메일을 포함합니다.
- **headers**: 인증을 위해 API 키를 API에 전달하는 `Authorization` 헤더를 포함합니다.

### 6단계: 요청 보내기 및 응답 처리

이제 Proxycurl API에 요청을 보내고 응답을 처리합니다:

```python
response = requests.get(api_endpoint, params=params, headers=headers)
```

이 줄은 지정된 매개변수와 헤더로 API에 GET 요청을 보냅니다. API는 JSON 형식으로 프로필 데이터를 반환합니다.

다음으로 요청이 성공했는지 확인하고 데이터를 저장합니다:

```python
if response.status_code == 200:
    profile_data = response.json()
    with open("profile_data.json", "w") as json_file:
        json.dump(profile_data, json_file, indent=4)
else:
    print(f"Error: {response.status_code}")
```

- **response.status_code**: 요청 상태를 확인합니다. 상태 코드 200은 요청이 성공했음을 의미합니다.
- **profile_data.json**: 성공하면 프로필 데이터가 `profile_data.json`이라는 파일에 저장됩니다.

### 7단계: 스크립트 실행

스크립트를 `app.py`로 저장하고 다음 명령으로 실행합니다:

```bash
python app.py
```

스크립트를 실행하면 디렉토리에 `profile_data.json`이라는 파일이 생성됩니다. 이 파일에는 LinkedIn 프로필에서 스크래핑한 모든 데이터가 구조화된 형식으로 포함되어 있습니다.

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1724180047004/7429386a-6b3c-43c2-a0b5-44f81ea29d6e.png)

### 결론

이상입니다! Python과 Proxycurl API를 사용하여 LinkedIn 프로필에서 데이터를 스크래핑하는 데 성공했습니다. 다음 튜토리얼에서는 이 스크래핑한 데이터를 사용하여 AI 생성 ATS 친화적 이력서를 만드는 방법을 탐구합니다.

전체 코드는 [GitHub](https://github.com/ayush-that/ResumeGenie)에서 확인할 수 있습니다.

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1724179604343/0e0ee533-6b25-47b4-9207-8e1afd41a82a.png)
