---
title: "PythonとProxycurl APIでLinkedInをスクレイピング"
description: PythonとProxycurl APIを使ってLinkedInプロフィールをスクレイピングする方法を学ぶ。セットアップ、API設定、レスポンス処理を含む
tags: ["web-scraping", "python"]
date: 2024-08-21
published: true
cover: "./images/cover/linkedin-scraping-with-python-and-proxycurl-api.avif"
---

LinkedInプロフィールからデータをスクレイピングすることは、リサーチから履歴書作成まで、さまざまな目的に非常に役立ちます。この記事では、PythonとProxycurl APIを使用してLinkedInプロフィールからデータをスクレイピングするプロセスを説明します。

### ステップ1：Proxycurlアカウントのセットアップ

コードに入る前に、[Proxycurlのウェブサイト](https://nubela.co/proxycurl/)でアカウントを作成する必要があります。登録後、Proxycurl APIにアクセスするために必要なAPIキーを受け取ります。

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1724180135536/7cb08f64-b07f-4983-b7b2-a69c480f9ed4.png)

### ステップ2：APIキーを安全に保存

APIキーを安全に保つために、`.env`ファイルに保存するのが良い方法です。このファイルは、APIキーなどの機密情報をコードベースから分離するために使用されます。`.env`ファイルのセットアップ方法：

```plaintext
API_KEY=your_api_key_here
```

`your_api_key_here`をProxycurlから受け取った実際のAPIキーに置き換えてください。

### ステップ3：Pythonスクリプトの作成

では、Pythonコードをステップバイステップで見ていきましょう。まず、必要なライブラリをインポートする必要があります：

```python
import requests
import json
from dotenv import load_dotenv
import os
```

- **requests**：PythonでHTTPリクエストを送信するためのライブラリで、Proxycurl APIとのやり取りに使用します。
- **json**：APIが返すJSON形式のデータを処理するために使用します。
- **dotenv**：`.env`ファイルから環境変数を読み込むのに役立ちます。
- **os**：環境変数へのアクセスなど、オペレーティングシステムとのやり取りのための関数を提供します。

次に、`.env`ファイルからAPIキーを読み込みます：

`load_dotenv()`関数は`.env`ファイルから環境変数を読み込み、`os.getenv("API_KEY")`は`API_KEY`変数の値を取得します。

### ステップ4：LinkedInプロフィールURLとAPIエンドポイントの設定

スクレイピングしたいLinkedInプロフィールのURLとProxycurl APIエンドポイントを指定する必要があります：

```python
linkedin_profile_url = "https://www.linkedin.com/in/ayush-that/"
api_endpoint = "https://nubela.co/proxycurl/api/v2/linkedin"
```

- **linkedin_profile_url**：スクレイピングしたいLinkedInプロフィールのURLです。
- **api_endpoint**：LinkedInプロフィールデータをスクレイピングするためのProxycurl APIエンドポイントです。

### ステップ5：APIリクエストの設定

スクレイピングしたいデータをカスタマイズするには、APIリクエストのパラメータとヘッダーを設定する必要があります：

```python
params = {
    "url": linkedin_profile_url,
    "fallback_to_cache": "on-error",
    "use_cache": "if-present",
    "skills": "include",
    "inferred_salary": "include",
    "extra": "include",
    "personal_email": "include",
}
headers = {
    "Authorization": f"Bearer {API_KEY}",
}
```

- **params**：これらのパラメータはAPIが返すデータを制御します。例えば、`skills`はプロフィールのスキルを含め、`inferred_salary`はプロフィールの給与推定を提供し、`personal_email`は利用可能な場合にメールを含めます。
- **headers**：認証のためにAPIキーをAPIに渡す`Authorization`ヘッダーを含みます。

### ステップ6：リクエストの送信とレスポンスの処理

Proxycurl APIにリクエストを送信し、レスポンスを処理します：

```python
response = requests.get(api_endpoint, params=params, headers=headers)
```

この行は指定されたパラメータとヘッダーでAPIにGETリクエストを送信します。APIはJSON形式でプロフィールデータを返します。

次に、リクエストが成功したかどうかを確認し、データを保存します：

```python
if response.status_code == 200:
    profile_data = response.json()
    with open("profile_data.json", "w") as json_file:
        json.dump(profile_data, json_file, indent=4)
else:
    print(f"Error: {response.status_code}")
```

- **response.status_code**：リクエストのステータスを確認します。ステータスコード200はリクエストが成功したことを意味します。
- **profile_data.json**：成功した場合、プロフィールデータは`profile_data.json`というファイルに保存されます。

### ステップ7：スクリプトの実行

スクリプトを`app.py`として保存し、次のコマンドで実行します：

```bash
python app.py
```

スクリプトを実行すると、ディレクトリに`profile_data.json`というファイルが見つかります。このファイルには、LinkedInプロフィールからスクレイピングしたすべてのデータが構造化された形式で含まれています。

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1724180047004/7429386a-6b3c-43c2-a0b5-44f81ea29d6e.png)

### 結論

以上です！PythonとProxycurl APIを使用してLinkedInプロフィールからデータをスクレイピングすることに成功しました。次のチュートリアルでは、このスクレイピングしたデータを使用してAI生成のATS対応履歴書を作成する方法を探ります。

完全なコードについては、[GitHub](https://github.com/ayush-that/ResumeGenie)をご覧ください。

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1724179604343/0e0ee533-6b25-47b4-9207-8e1afd41a82a.png)
